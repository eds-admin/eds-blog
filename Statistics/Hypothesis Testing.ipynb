{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Hypothesis-Testing\" data-toc-modified-id=\"Hypothesis-Testing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Hypothesis Testing</a></div><div class=\"lev3 toc-item\"><a href=\"#A-note-before-we-begin:\" data-toc-modified-id=\"A-note-before-we-begin:-101\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>A note before we begin:</a></div><div class=\"lev3 toc-item\"><a href=\"#Acknowledgements:\" data-toc-modified-id=\"Acknowledgements:-102\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Acknowledgements:</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Null-Hypothesis\" data-toc-modified-id=\"The-Null-Hypothesis-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The Null Hypothesis</a></div><div class=\"lev2 toc-item\"><a href=\"#The-normal-distribution\" data-toc-modified-id=\"The-normal-distribution-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>The normal distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#The-p-value\" data-toc-modified-id=\"The-p-value-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>The $p$-value</a></div><div class=\"lev3 toc-item\"><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Interpretation</a></div><div class=\"lev3 toc-item\"><a href=\"#Making-mistakes\" data-toc-modified-id=\"Making-mistakes-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Making mistakes</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "### A note before we begin:\n",
    "Hypothesis testing is a rather complicated mathematical process. It involves familiar knowledge with various distribution functions, especially the normal distribution function and ways to generate values from it with Cumulative Distribution Functions or Probability Distribution Functions. This notebook will move quickly to give a point of reference for how this kind of thing can be done \"under the hood\", but in all likelihood you will be using a library like `scipy.stats` to do this quickly and easily. So just hang on for the ride.\n",
    "\n",
    "### Acknowledgements:\n",
    "A lot of the statistical functions were directly pulled from the book _Data Science from Scratch_. A fun book that's really well suited for this site, but not necessary reading to become a good data scientist. Nonetheless, credit is given where due.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Perhaps one of the most useful, and certainly a concept in which you should be very fluent, is hypothesis testing. A lot of what scientists parade as fact are better described as \"statistically significant.\" In Physics, gravity will pull something down 100% of the time. In the real world, however, there is significant randomness (which is a topic for another notebook).\n",
    "\n",
    "Statistical inference, which is the process of inferring a distribution or a feature from sample observations, is a big an important field. Today, we're just going to talk about testing hypotheses to make sure our findings are statistically significant.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "## The Null Hypothesis\n",
    "You've heard this before. The basic process of confidence testing is we put forth a _null hypothesis_ and ask the data to provide enough evidence that we reject this _null hypothesis_. The term _null_ signifies that the difference between the variables we are testing is _null_.\n",
    "\n",
    "If the data does not provide sufficient evidence (or what we deem sufficient, typically with a score called a $p$-value) then we retain the null hypothesis. Note: we do not __accept__ the null hypothesis, we just __fail to reject__ the null hypothesis. Same same, but different.\n",
    "\n",
    "The two variables we're testing are typically called the _null_ and _alternative_ hypotheses. In statistics, they are denoted $H_0$ and $H_A$ respectively. Here's a simple example with coin flips:\n",
    "\n",
    "* $H_0$: The coin is fair (has a probability $p=\\frac{1}{2}$\n",
    "* $H_A$: The coin is not fair (has a probability $p\\ne\\frac{1}{2}$\n",
    "\n",
    "So let's try this out by flipping a coin a bunch of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads p=0.486\n",
      "Tails p=0.514\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def flip(n):\n",
    "    results ={'heads': 0, 'tails': 0}\n",
    "    for _ in range(n):\n",
    "        coin = random.choice(['heads', 'tails'])\n",
    "        results[coin] += 1\n",
    "    return results\n",
    "\n",
    "def print_proportions(flips):\n",
    "    total = flips['heads'] + flips['tails']\n",
    "    print(\"Heads p={}\\nTails p={}\".format(\n",
    "        flips['heads']/total, flips['tails']/total))\n",
    "\n",
    "print_proportions(flip(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we flipped the coin 1,000 times, we still didn't get perfect $p$-values of 0.5 each. So we want to test our null hypothesis by saying \"let us assume that the probability of heads and tails are both 0.5. What are the odds that we then get the results we did? And is it so unlikely that we have to reject the hypothesis that the two coins have the same probability?\"\n",
    "\n",
    "## The normal distribution\n",
    "Remember our brief discussion of the normal distribution in descriptive statistics? We haven't gone deep into distributions yet, but here are some spoilers that you can feel free to take at face value:\n",
    "\n",
    "* The number of heads in an experiment of $n$ flips of a coin with probability $p$ will be normally distributed with mean $\\mu = p*n$ and $\\sigma = \\sqrt{p*(1-p)*n}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 15.811388300841896)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def normal_approx_to_binomial(n, p):\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p)  * n)\n",
    "    return mu, sigma\n",
    "\n",
    "normal_approx_to_binomial(1000, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To find the probability that the number of heads is below a certain value, we use a Cumulative Distribution Function, or CDF. It simply says that given a $\\mu$, $\\sigma$, value $X$, and observed result $x$, what is $P(x \\ge X)$?\n",
    "* We can use pythons `math.erf()` function to create a normal CDF (just hang on for the ride here, this is not the detailed subject of this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_cdf(x, mu=0, sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with this for a bit so you get a sense. Let's use our coin flip example were $\\mu=500$ and $\\sigma=15.81$ and see the probability that the number of heads is greater than various values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x <= 450) = 0.00078\n",
      "P(x <= 475) = 0.05692\n",
      "P(x <= 490) = 0.26354\n",
      "P(x <= 500) = 0.50000\n",
      "P(x <= 510) = 0.73646\n",
      "P(x <= 525) = 0.94308\n",
      "P(x <= 550) = 0.99922\n"
     ]
    }
   ],
   "source": [
    "mu, sigma = normal_approx_to_binomial(1000, 0.5)\n",
    "\n",
    "for x in [450, 475, 490, 500, 510, 525, 550]:\n",
    "    print(\"P(x <= {}) = {:0.5f}\".format(x, normal_cdf(x, mu, sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the output above, what does it tell you? The probability that our observed value, $x$, is less than or equal to our test value, $X$. So in the case of 450, there is a 0.08% chance that we get less than 450 heads after 1,000 flips. That's pretty low. So we would be super confident that if we ran the trial and got 450 flips there's something strange going on with that coin.\n",
    "\n",
    "That is the fundamentals of hypothesis testing. Now let's talk a bit more about that percentage number and what it means to statisticians.\n",
    "\n",
    "## The $p$-value\n",
    "The numbers we caluclated in the previous cell are referred to as _probability values_ (or $p$-values) by statisticians. They are crucially important. Most scientific studies that tout a finding to be statistically significant are technically saying they got a low enough $p$-value to reject the null hypothesis.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Stop for a second and think about what it means in technical language: _the probability that our sample statistic is the result of chance is lower than some given threshold, thus we assert it was not sourced from the model distribution_.\n",
    "\n",
    "And just as important is the reverse: _the probability that our sample statistic is the result of chance is higher than a given threshold, thus we retain the alternative hypothesis_.\n",
    "\n",
    "This language, _thus we retain the alternative hypothesis_, is very important. **We are not saying that the two are the same, just that we cannot be certain they are the same**. In fact, very few things in statistics are the same unless they come from near-perfect random distributions like dice and coins and playing cards. When we talk about people, there's a lot more blur aorund the edges.\n",
    "\n",
    "### Making mistakes\n",
    "\n",
    "Two very important problems result now in hypothesis testing: to get the model distribution (sometimes called the _test statistic_) correct and to pick a reasonable threshold, or $p$-value.\n",
    "\n",
    "If these are wrong, there are two types of errors the experimenters will be more subject to:\n",
    "* **Type I**: Rejecting the Null Hypothesis when it is in fact true\n",
    "* **Type II**: Retaining the Null Hypothesis when it is in fact false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
